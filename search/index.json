[{"content":"DNS 和域名 一、域名 网域名称（英语：Domain Name，简称：Domain），简称域名、网域，是由一串用点分隔的字符组成的互联网上某一台计算机或计算机组的名称，用于在数据传输时标识计算机的电子方位。域名可以说是一个IP地址的代称，目的是为了便于记忆后者。\n域名的层级 www.example.com 真正的域名是 www.example.com.root ，简写为 www.example.com. 。因为，根域名 .root 对于所有域名都是一样的，所以平时是省略的。\n根域名的下一级，叫做顶级域名（top-level domain，缩写为TLD），比如 .com 、 .net ；再下一级叫做次级域名（second-level domain，缩写为SLD），比如 www.example.com 里面的 .example ，这一级域名是用户可以注册的；再下一级是主机名（host），比如www.example.com里面的www，又称为三级域名，这是用户在自己的域里面为服务器分配的名称，是用户可以任意分配的。\n 主机名.次级域名.顶级域名.根域名 host.sld.tld.root\n 二、DNS 域名系统（英语：Domain Name System，缩写：DNS）是一个分布式数据库，提供了域名和 IP地址之间相互转换的服务。\n查询过程 DNS 服务器根据域名的层级，进行分级查询。\n每一级域名都有自己的NS（Name Server） 记录，NS记录指向该级域名的域名服务器。这些服务器知道下一级域名的各种记录。\n \u0026ldquo;分级查询\u0026rdquo;，就是从根域名开始，依次查询每一级域名的NS记录，直到查到最终的IP地址\n过程大致为:\n 从\u0026quot;根域名服务器\u0026quot;查到\u0026quot;顶级域名服务器\u0026quot;的NS记录和A记录（IP地址） 从\u0026quot;顶级域名服务器\u0026quot;查到\u0026quot;次级域名服务器\u0026quot;的NS记录和A记录（IP地址） 从\u0026quot;次级域名服务器\u0026quot;查出\u0026quot;主机名\u0026quot;的IP地址   dig math.stackexchange.com ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.11.3-1ubuntu1.15-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; math.stackexchange.com ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 50719 ;; flags: qr rd ra; QUERY: 1, ANSWER: 4, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 65494 ;; QUESTION SECTION: ;math.stackexchange.com. IN A ;; ANSWER SECTION: math.stackexchange.com. 600 IN A 151.101.193.69 math.stackexchange.com. 600 IN A 151.101.129.69 math.stackexchange.com. 600 IN A 151.101.65.69 math.stackexchange.com. 600 IN A 151.101.1.69 ;; Query time: 74 msec ;; SERVER: 127.0.0.53#53(127.0.0.53) ;; WHEN: Mon Sep 13 10:18:12 UTC 2021 ;; MSG SIZE rcvd: 115 记录类型 域名与IP之间的对应关系，称为记录（record）。根据使用场景，记录可以分成不同的类型（type）。\n   记录类型 记录类型简称 描述     地址记录（Address） A 返回域名指向的IP地址   域名服务器记录（Name Server） NS 返回保存下一级域名信息的服务器地址。该记录只能设置为域名，不能设置为IP地址   邮件记录（Mail eXchange） MX 返回接收电子邮件的服务器地址   规范名称记录（Canonical Name） CNAME 返回另一个域名，即当前查询的域名是另一个域名的跳转   逆向查询记录（Pointer Record） PTR 只用于从IP地址查询域名    传输方式 DNS 可以使用 UDP 或者 TCP 进行传输，使用的端口号都为 53。大多数情况下 DNS 使用 UDP 进行传输，这就要求域名解析器和域名服务器都必须自己处理超时和重传从而保证可靠性。在两种情况下会使用 TCP 进行传输：\n 如果返回的响应超过的 512 字节（UDP 最大只支持 512 字节的数据） 区域传送（区域传送是主域名服务器向辅助域名服务器传送变化的那部分数据）  ","date":"2020-06-14T16:02:15+08:00","permalink":"https://huangkai1008.github.io/p/http/","title":"HTTP"},{"content":"HTTP 一、概述 请求和响应报文 客户端发送一个请求报文给服务器，服务器根据请求报文中的信息进行处理，并将处理结果放入响应报文中返回给客户端。\n请求消息（requests） GET / HTTP/1.1 Host: developer.mozilla.org Accept-Language: fr   起始行（start line）：包含一个HTTP方法（method）、请求目标（request target） 和 HTTP 版本 （HTTP version）\n  消息头（headers）： 整个 header（包括其值）表现为单行形式\n  一个空行用来分隔首部和内容主体 Body\n  消息主体（body）\n   响应消息(responses) HTTP/1.1 200 OK Date: Sat, 09 Oct 2010 14:28:02 GMT Server: Apache Last-Modified: Tue, 01 Dec 2009 20:18:22 GMT ETag: \u0026#34;51142bc1-7449-479b075b2891b\u0026#34; Accept-Ranges: bytes Content-Length: 29769 Content-Type: text/html \u0026lt;!DOCTYPE html... (here comes the 29769 bytes of the requested web page)  状态行（status line)：  协议版本，通常为 HTTP/1.1. 状态码 (status code)，表明请求是成功或失败。常见的状态码是 200，404，或 302 状态文本 (status text)：一个简短的，纯粹的信息，通过状态码的文本描述，帮助理解该 HTTP 消息   消息头（Headers）： 整个 header（包括其值）表现为单行形式 一个空行用来分隔首部和内容主体 Body 消息主体（body）   二、HTTP 方法    请求方法 描述 RFC 请求具有请求实体 响应具有响应实体 安全方法 是否幂等     GET 请求一个指定的资源 RFC 7231 可选 是 是 是   HEAD 获取报文首部，不返回报文实体主体，主要用于确认 URL 的有效性以及资源更新的日期时间等 RFC 7231 可选 否 是 是   POST 用于将实体提交到指定的资源 RFC 7231 是 是 否 否   PUT 向指定资源位置上传其最新内容 RFC 7231 是 是 否 是   PATCH 对资源进行部分修改 RFC 5789 是 是 否 否   DELETE 删除指定的资源 RFC 7231 可选 是 否 是   CONNECT 要求在与代理服务器通信时建立隧道 RFC 7231 可选 是 否 否   OPTIONS 查询指定的 URL 能够支持的方法 RFC 7231 可选 是 是 是   TRACE 服务器会将通信路径返回给客户端 RFC 7231 否 是 是 是     三、HTTP 首部（header)    类型 描述 实例     通用头（General headers） 适用于请求和响应信息的头字段 Date,Cache-Control   请求头（Request headers） 用于表示请求信息的附加信息的头字段 Authorization,User-Agent,Accept-Encoding   响应头（Response headers） 用于表示响应信息的附加信息的头字段 Location,Server   实体头（Entity headers） 用于表示实体（消息体）的附加信息的头字段 Allow,Content-Encoding,Expires, Etag    四、HTTP 状态码（status code）    状态码 含义     1xx(informational response) 告知请求的处理进度和情况   2xx(successful) 成功   3xx(redirection) 需要进一步处理   4xx(client error) 客户端错误   5xx(server error) 服务器错误    参考资料  【日】户根勤. (2017). 网络是怎样连接的. 人民邮电出版社. Wikipedia : HTTP MDN : HTTP  ","date":"2020-06-12T12:22:57+08:00","permalink":"https://huangkai1008.github.io/p/http/","title":"HTTP"},{"content":"日志类型  ​\tMySQL主要有两种日志类型，一种是物理日志（记录在某个数据页上做了什么修改)，一种是逻辑日志(存储了逻辑SQL修改语句)。redo log属于物理日志，binlog和undo log属于逻辑日志，其中物理日志的恢复速度远快于逻辑日志。\nredo log和undo log都属于InnoDB引擎层下的事务日志（transaction log）。\n 重做日志 (redo log) 预写式技术 (Write Ahead logging, WAL)  ​ InnoDB 引擎对数据更新，是先将更新记录写入到重做日志，在系统空闲时或者按照设定的更新策略再将日志中的内容更新到磁盘中，这就是预写式技术 (Write Ahead logging, WAL)，这种技术可以大大减少IO操作的频率，提升数据刷新的效率。\n 重做日志的策略  ​\t重做日志（redo log）是 InnoDB 引擎层的日志，用来记录事务操作引起数据的变化，记录的是数据页的物理修改，提供前滚操作，redo log 保证事务的持久性。重做日志由两部分组成，一是内存中的重做日志缓冲区 (redo log buffer)，因为重做日志缓冲区在内存中，所以它是易失的，另一个就是在磁盘上的重做日志文件 (redo log file)，它是持久的。\n ​\t当我们在一个事务中尝试对数据进行修改时，它会先将数据从磁盘读入内存，并更新内存中缓存的数据，然后生成一条重做日志并写入重做日志缓存，当事务真正提交时，MySQL 会将重做日志缓存中的内容刷新到重做日志文件，再将内存中的数据更新到磁盘上。\n​\t值得注意的是，redo log 的大小是固定的，为了能够持续不断的对更新记录进行写入，在redo log日志中设置了两个标志位置，checkpoint和write pos。checkpoint表示记录擦除的位置，write pos表示记录写入的位置。当write pos标志到了日志结尾时，会从结尾跳至日志头部循环写入，所以redo log的逻辑结构并不是线性的，可以看做一个圆周运动，逻辑结构见下图：\n​\t当write_pos追上checkpoint时，表示redo log日志已经写满。这时不能继续执行新的数据库更新语句，需要停下来先删除一些记录，执行checkpoint规则腾出可写空间。\n checkpoint规则: checkpoint触发后，将buffer中脏数据页和脏日志页都刷到磁盘。所谓的脏数据页就是指内存中未刷到磁盘的数据\n ​ redo log中最重要的概念就是缓冲池buffer pool，这是在内存中分配的一个区域，包含了磁盘中部分数据页的映射，作为访问数据库的缓冲。当请求读取数据时，会先判断是否在缓冲池命中，如果未命中才会在磁盘上进行检索后放入缓冲池；当请求写入数据时，会先写入缓冲池，缓冲池中修改的数据会定期刷新到磁盘中。这一过程也被称之为刷脏 。\n​\t因此，当数据修改时，除了修改buffer pool中的数据，还会在redo log中记录这次操作；当事务提交时，会根据redo log的记录对数据进行刷盘。如果MySQL宕机，重启时可以读取redo log中的数据，对数据库进行恢复，从而保证了事务的持久性，使得数据库获得crash-safe能力。\n回滚日志 (undo log)  ​\t回滚日志的作用就是对数据进行回滚。当事务对数据库进行修改，InnoDB引擎不仅会记录redo log，还会生成对应的undo log日志；如果事务执行失败或调用了rollback，导致事务需要回滚，就可以利用undo log中的信息将数据回滚到修改之前的状态。\n​ 但是undo log和redo log不一样，它属于逻辑日志。它对SQL语句执行相关的信息进行记录。当发生回滚时，InnoDB引擎会根据undo log日志中的记录做与之前相反的工作。比如对于每个数据插入操作（insert），回滚时会执行数据删除操作（delete）；对于每个数据删除操作（delete），回滚时会执行数据插入操作（insert）；对于每个数据更新操作（update），回滚时会执行一个相反的数据更新操作（update），把数据改回去。undo log有两个作用，一是提供回滚，二是实现MVCC。\n 二进制日志 (binlog)  ​ 二进制日志binlog是服务层的日志，还被称为归档日志。binlog主要记录数据库的变化情况，内容包括数据库所有的更新操作。所有涉及数据变动的操作，都要记录进二进制日志中。因此有了binlog可以很方便的对数据进行复制和备份，因而也常用作主从库的同步。\n ","date":"2020-06-05T22:07:21+08:00","permalink":"https://huangkai1008.github.io/p/mysql%E7%9A%84%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/","title":"MySQL的日志系统"},{"content":"概念  事务就是一组原子性的SQL查询，或者说一个独立的工作单元。如果数据库引擎能够成功地对数据库应用该组查询的全部语句，那么就执行该组查询。如果其中有任何一条语句因为崩溃或其他原因无法执行，那么所有的语句都不会执行。也就是说，事务内的语句，要么全部执行成功，要么全部执行失败。在 MySQL 中，事务支持是在引擎层实现的。\n ACID特性   原子性（Atomicity）：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行\n  一致性（Consistency）：事务应确保数据库的状态从一个一致状态转变为另一个一致状态。一致状态的含义是数据库中的数据应满足完整性约束。\n  隔离性（Isolation）：通常来说，一个事务所做的修改在最终提交以前，对其他事务是不可见的。\n  持久性（Durability）：已被提交的事务对数据库的修改应该永久保存在数据库中。\n   一个实现了ACID的数据库，相比没有实现ACID的数据库，通常会需要更强的CPU处理能力、更大的内存和更多的磁盘空间。\n 隔离级别（Isolation level）   READ UNCOMMITTED（读未提交）\n​\t事务中的修改，即使没有提交，对其他事务也都是可见的。事务可以读取未提交的数据，这也被称为脏读（Dirty Read）。\n  READ COMMITTED（读提交）\n​\tOracle和SQL Server的默认隔离级别。一个事务可以读取另一个已提交的事务。换句话说，一个事务从开始直到提交之前，所做的任何修改对其他事务都是不可见的。这个级别有时候也叫做不可重复读（nonrepeatable read），因为两次执行同样的查询，可能会得到不一样的结果。MySQL的InnoDB引擎在提交读级别通过MVCC解决了不可重复读的问题\n  REPEATABLE READ（可重复读）\n​\tMySQL的默认隔离级别。一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。MySQL的InnoDB引擎在提交读级别通过MVCC解决了不可重复读的问题，在可重复读级别通过间隙锁解决了幻读问题。\n  SERIALIZABLE（可串行化）\n​\tSERIALIZABLE是最高的隔离级别。它通过强制事务串行执行，避免了前面说的幻读的问题。简单来说，SERIALIZABLE会在读取的每一行数据上都加锁，所以可能导致大量的超时和锁争用的问题。实际应用中也很少用到这个隔离级别，只有在非常需要确保数据的一致性而且可以接受没有并发的情况下，才考虑采用该级别。\n     隔离级别 脏读可能性 不可重复读可能性 幻读可能性 加锁读     READ UNCOMMITTED √ √ √ ×   READ COMMITTED × √ √ ×   REPEATABLE READ × × √ ×   SERIALIZABLE × × × √     查看MySQL的隔离级别\n SHOW VARIABLES LIKE \u0026#39;transaction_isolation\u0026#39;; +-----------------------+-----------------+ | Variable_name | Value | +-----------------------+-----------------+ | transaction_isolation | REPEATABLE-READ | +-----------------------+-----------------+ 1 row in set, 1 warning (0.00 sec)  设置当前会话的隔离级别\n SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;\t# 设置当前会话为RC级别，下个事务生效 事务类型 隐式事务 DML操作的语句都会隐式的开启事务，并且在语句执行后没有错误的话隐式提交。可以通过将MySQL的autocommit这个变量（默认为1）设置为0将事务的隐式提交关闭，但需要注意，DML语句的隐式事务仍会启动，只是区别在于需要手动COMMIT显式提交这个事务，也就是将隐式事务转化为长事务了。\nSHOW VARIABLES LIKE \u0026#39;autocommit\u0026#39;; # 查看隐式事务提交方式  +---------------+-------+ | Variable_name | Value | +---------------+-------+ | autocommit | ON | +---------------+-------+ 1 row in set, 1 warning (0.00 sec) 显式事务 # 1.显式开启一个事务 START TRANSACTION; BEGIN; # 2.提交事务 COMMIT; # 3.回滚事务 ROLLBACK; # 4.在事务中创建保存点，可以在同一事务中创建多个，以便通过ROLLBACK更灵活的回滚 SAVEPOINT; 显式开启一个事务时，如果还有未提交的事务会自动提交，并且autocommit会被禁用直到该事务结束。对于显式事务，存在completion_type这样一个变量控制显式事务的行为。有下列三种情况：\n 值为0时即为默认，执行COMMIT后提交该显式事务并结束该事务。 值为1时，执行COMMIT后除了有值为0时的默认行为外，随后会自动开始一个相同隔离级别的事务。术语为COMMIT AND CHAIN 值为2时，执行COMMIT后除了有值为0时的默认行为外，随后会断开与服务器的连接。术语为COMMIT AND RELEASE  ","date":"2020-06-02T22:25:20+08:00","permalink":"https://huangkai1008.github.io/p/%E4%BA%8B%E5%8A%A1/","title":"事务"},{"content":"逻辑架构 MySQL可以大体分为Server层和存储引擎层两部分, 见图1\n Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎  连接器(Connector) ​\t连接器负责和客户端建立连接、获取权限、维持和管理连接。\n# Mysql连接命令 mysql -h$ip -P$port -u$user -p MySQL客户端和服务端完成TCP握手后，连接器需要认证身份\n 如果用户名或密码不对，你就会收到一个\u0026quot;Access denied for user\u0026quot;的错误，然后客户端程序结束执行。 如果用户名密码认证通过，连接器会到权限表里面查出拥有的权限，之后这个连接里面的权限判断逻辑，都将依赖于此时读到的权限，这就意味着，一个用户成功建立连接后，即使这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。  查询缓存(Query Cache) ​\t在连接建立完成后，MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果你的查询能够直接在这个缓存中找到 key，那么这个 value 就会被直接返回给客户端。\n MySQL 8.0 版本的查询缓存功能被移除了\n 分析器(Parser) 分析器的主要功能是对SQL语句做解析\n 分析器会先做词法分析，再做语法分析，语法分析器会根据语法规则，判断 SQL 语句是否满足 MySQL 语法  优化器(Query Optimizer) ​\t优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。优化器并不关心表使用的是什么存储引擎，但存储引擎对于优化查询是有影响的。优化器会请求存储引擎提供容量或某个具体操作的开销信息，以及表数据的统计信息等。例如，某些存储引擎的某种索引，可能对一些特定的查询有优化。\n执行器(Query execution engine) ​\t开始执行的时候，要先判断一下对于表有没有执行操作的权限，如果没有，就会返回没有权限的错误。如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。\n对于一个特定的例子\nselect * from user where ID=10; 假定ID字段没有索引，那么执行器的执行流程是这样的：\n 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中； 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。  ","date":"2020-06-01T21:56:20+08:00","permalink":"https://huangkai1008.github.io/p/mysql%E7%9A%84%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84/","title":"MySQL的基础架构"},{"content":"IDE  Jetbrains系列  Editor  Vscode Sublime text Typora(markdown编辑) Notion(笔记软件)  画图  Microsoft visio drawing.io  数据库 Mysql系列  Navicat Tableplus Jetbrains Datagrip  Redis系列  RDM(redis desktop manager)  CVS  Jetbrains系IDE自带 SourceTree  接口测试  Postman  Vm  VirtualBox Vagrant Windows SubLinux(WSL) Vmware  ssh工具  MobaXterm xshell  系统工具  Everything Utools  接口文档工具  apidoc swagger yapi  终端  cmder Windows Termial  ","date":"2019-12-01T22:21:57+08:00","permalink":"https://huangkai1008.github.io/p/awesome-software/","title":"Awesome Software"},{"content":"Loguru是一个好用的第三方python日志库\n安装 pip install loguru 初步使用 添加日志到标准输出流 import sys from loguru import logger logger.add(sys.stderr, format=\u0026#39;{time} {level} {message}\u0026#39;, filter=\u0026#39;my module\u0026#39;, level=\u0026#39;INFO\u0026#39;) 添加日志到文件 from loguru import logger logger.add(\u0026#39;file_1.log\u0026#39;, rotation=\u0026#39;500 MB\u0026#39;) # Automatically rotate too big file logger.add(\u0026#34;file_2.log\u0026#34;, rotation=\u0026#39;12:00\u0026#39;) # New file is created each day at noon logger.add(\u0026#34;file_3.log\u0026#34;, rotation=\u0026#34;1 week\u0026#34;) # Once the file is too old, it\u0026#39;s rotated logger.add(\u0026#34;file_X.log\u0026#34;, retention=\u0026#34;10 days\u0026#34;) # Cleanup after some time logger.add(\u0026#34;file_Y.log\u0026#34;, compression=\u0026#34;zip\u0026#34;) # Save some loved space 捕获异常 from loguru import logger @logger.catch def my_function(x, y, z): # An error? It\u0026#39;s caught anyway! return 1 / (x + y + z) 为日志添加颜色 import sys from loguru import logger logger.add(sys.stdout, colorize=True, format=\u0026#34;\u0026lt;green\u0026gt;{time}\u0026lt;/green\u0026gt; \u0026lt;level\u0026gt;{message}\u0026lt;/level\u0026gt;\u0026#34;) 异步、线程安全、多进程安全 from loguru import logger logger.add(\u0026#34;file.log\u0026#34;, enqueue=True) 完全描述异常  记录代码中发生的异常对于跟踪错误很重要，但是如果您不知道为什么失败，则记录日志就毫无用处。 Loguru通过允许显示整个堆栈跟踪（包括变量值）来帮助您发现问题\n from loguru import logger logger.add(\u0026#34;output.log\u0026#34;, backtrace=True, diagnose=True) # Set \u0026#39;False\u0026#39; to not leak sensitive data in prod 配置到flask import logging import sys from pathlib import Path from flask import Flask from loguru import logger app = Flask(__name__) class InterceptHandler(logging.Handler): def emit(self, record): logger_opt = logger.opt(depth=6, exception=record.exc_info) logger_opt.log(record.levelname, record.getMessage()) def configure_logging(flask_app: Flask): \u0026#34;\u0026#34;\u0026#34;配置日志\u0026#34;\u0026#34;\u0026#34; path = Path(flask_app.config[\u0026#39;LOG_PATH\u0026#39;]) if not path.exists(): path.mkdir(parents=True) log_name = Path(path, \u0026#39;sips.log\u0026#39;) logging.basicConfig(handlers=[InterceptHandler(level=\u0026#39;INFO\u0026#39;)], level=\u0026#39;INFO\u0026#39;) logger.configure(handlers=[{\u0026#34;sink\u0026#34;: sys.stderr, \u0026#34;level\u0026#34;: \u0026#39;INFO\u0026#39;}]) # 配置日志到标准输出流 logger.add( log_name, rotation=\u0026#34;500 MB\u0026#34;, encoding=\u0026#39;utf-8\u0026#39;, colorize=False, level=\u0026#39;INFO\u0026#39; ) # 配置日志到输出到文件 ","date":"2019-11-22T15:19:35+08:00","permalink":"https://huangkai1008.github.io/p/%E6%97%A5%E5%BF%97%E5%BA%93loguru%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/","title":"日志库Loguru使用教程"},{"content":"Evenlet是一个Python的基于携程的网络库，它改变了你代码运行的方式，但是没有改变你怎么写代码\n安装 pip install eventlet 简单使用 从eventlet.green导入相关库 import eventlet from eventlet.green import urllib2 urls = [ \u0026#34;https://www.google.com/intl/en_ALL/images/logo.gif\u0026#34;, \u0026#34;http://python.org/images/python-logo.gif\u0026#34;, \u0026#34;http://us.i1.yimg.com/us.yimg.com/i/ww/beta/y3.gif\u0026#34;, ] def fetch(url): print(\u0026#34;opening\u0026#34;, url) body = urllib2.urlopen(url).read() print(\u0026#34;done with\u0026#34;, url) return url, body pool = eventlet.GreenPool(200) for url, body in pool.imap(fetch, urls): print(\u0026#34;got body from\u0026#34;, url, \u0026#34;of length\u0026#34;, len(body)) 使用spawn使用协程 import time import eventlet def green_thread_1(num): eventlet.greenthread.sleep(1) print(f\u0026#39;green_thread_1 get result {num}\u0026#39;) return x def green_thread_2(num): eventlet.greenthread.sleep(2) print(f\u0026#39;green_thread_2 get result {num}\u0026#39;) return y time1 = time.perf_counter() x = eventlet.spawn(green_thread_1, 1) y = eventlet.spawn(green_thread_2, 2) x.wait() y.wait() time2 = time.perf_counter() print(time2 - time1) \u0026gt;\u0026gt;\u0026gt; green_thread_1 get result 1 green_thread_2 get result 2 2.0049271  spawn函数产生的协程可以通过wait函数来执行并获取返回结果， 如上例子中， 使用绿色线程的休眠模拟io操作的耗时, 程序就会切换到下一个协程，切换协程由调度器决定\n 使用monkey-patch from eventlet import monkey_patch from eventlet import GreenPool green_pool = GreenPool() monkey_patch() def producer(): pass def consumer(): pass green_pool.spawn(producer) green_pool.spawn(consumer) green_pool.waitall() 和gunicorn一起使用 以flask应用为例\ngunicorn --worker-class eventlet -b 0.0.0.0:5000 -w 1 run:app ","date":"2019-11-22T11:20:20+08:00","permalink":"https://huangkai1008.github.io/p/eventlet%E4%BD%BF%E7%94%A8/","title":"Eventlet使用"},{"content":"GitFlow 基本介绍 Gitflow 提倡使用 feature branches 模式来开发各个相互独立的功能，同时分成不同的分支以便进行集成和发布\n分支介绍   长期分支\n   主分支(master)\n  开发分支(develop)\n   在gitflow下, develop 分支是一个类似全能的分支，用来存放、测试所有的代码，同时也是主要是用来合并代码、集成功能的分支\n作为一个开发人员，在这是不允许直接提交代码到 develop 分支上的，更更更不允许直接提交到 master 分支。master 分支代表的是一个「stable」的分支，包含的是已投产或即将投产的代码。如果一段代码在 master 分支上，即代表它已经投产或即将投产发布\n  短期分支\n   功能分支(feature)\n  热补丁分支(hotfix)\n  预发分支(release)\n     feature\n功能性分支从 develop 分支上产生， 根据新需求来新建 feature 分支， 开发完成后，要再并入 develop 分支， 合并完分支后一般会删除这个feature分支\n在 feature 分支的命名规则上，可以约定以 「feat-」开头，后面跟上问题单编号。如「feat-APS-151-add-name-field」。以「feat-」开头，可以让 CI 服务器识别出这是一个 feature 分支，「APS-151」是Jira 问题单的编号，可以链接到问题单，剩下的部分则是对该功能的简短的说明\n  release\nrelease分支基于develop创建\n打完release分支之后，我们可以在这个release分支上测试，修改bug等。同时，其它开发人员可以基于develop分支新建feature (记住：一旦打了release分支之后不要从develop分支上合并新的改动到release分支)发布release分支时，合并release到master和develop， 同时在master分支上打个tag记住release版本号，然后可以删除release分支了。它的命名，可以采用release-*的形式\n在测试中，难免发现 bug，我们可以直接在 release 分支上修改，修改完后再 merge 到 develop 分支上（develop 分支包含的是已发布或者即将发布的代码）\n  hotfix\n这个分支是负责在生产环境上发现的问题，如 bug 或者性能问题等。 hotfixes 分支和 release 分支类似，都以 release 版本号命名，唯一的区别就是 hotfixes 是新建于 master 分支，release 分支则是从 develop 分支而来，修补结束以后，再合并进Master和Develop分支。它的命名，可以采用hotfix-*的形式\n    ​\n","date":"2019-10-12T14:36:49+08:00","permalink":"https://huangkai1008.github.io/p/git%E5%B7%A5%E4%BD%9C%E6%B5%81/","title":"Git工作流"},{"content":"Black是一个毫不妥协的python代码格式化工具, 特点是可配置项较少 Black依赖于python3.6+, 官方地址在https://github.com/psf/black\nInstall pip install black Configure   pyproject.toml\n[tool.black] skip-string-normalization = true # 禁用双引号风格   pycharm\n  Create external tools\n windows: File -\u0026gt; Settings -\u0026gt; Tools -\u0026gt; External Tools\n   Configure file watcher\n    ","date":"2019-09-27T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/black%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/","title":"Black安装和使用"},{"content":"安装   custom installer\ncurl -sSL https://raw.githubusercontent.com/sdispater/poetry/master/get-poetry.py | python source $HOME/.poetry/env   pip\npip install poetry # 不推荐, 可能会有冲突   验证安装\npoetry --version 使用   项目初始化\n  从pipenv/pip等工具迁移\npoetry init # 进入交互式命令行填写项目信息, 会生成pyproject.toml    添加依赖\n  添加包\npoetry add poetry add fastapi=0.38.1 -E all # pipenv install fastapi[all] poetry add celery --extras \u0026#34;librabbitmq redis auth msgpack\u0026#34; # pip install \u0026#34;celery[librabbitmq,redis,auth,msgpack]\u0026#34;   依赖安装\npoetry install # 会从pyproject.toml文件里读取, 如果有poetry.lock文件则会从lock文件中读取锁定依赖并安装   虚拟环境地址\n windows10: $User\\AppData\\Local\\pypoetry\\Cache\\virtualenvs      配置   添加源\n修改pyproject.toml\n[[tool.poetry.source]] name = \u0026#34;tsinghua\u0026#34; url = \u0026#34;https://pypi.tuna.tsinghua.edu.cn/simple/\u0026#34; verify_ssl = true   完整的实例 [tool.poetry] name = \u0026#34;market-admin\u0026#34; version = \u0026#34;0.1.0\u0026#34; description = \u0026#34;market-admin is a Market background management system with fastapi\u0026#34; authors = [\u0026#34;huangkai\u0026#34;] license = \u0026#34;MIT\u0026#34; [tool.poetry.dependencies] python = \u0026#34;^3.7\u0026#34; fastapi = {version = \u0026#34;0.38.1\u0026#34;, extras = [\u0026#34;all\u0026#34;]} python-dotenv = \u0026#34;0.10.2\u0026#34; tortoise-orm = \u0026#34;0.13.5\u0026#34; aiomysql = \u0026#34;0.0.20\u0026#34; loguru = \u0026#34;^0.3.2\u0026#34; [tool.poetry.dev-dependencies] pytest = \u0026#34;6.2.1\u0026#34; coverage = \u0026#34;5.3.1\u0026#34; [tool.black]\t# Black工具配置 target-version = [\u0026#39;py37\u0026#39;] skip-string-normalization = true [[tool.poetry.source]]\t# 源配置 name = \u0026#34;tsinghua\u0026#34; url = \u0026#34;https://pypi.tuna.tsinghua.edu.cn/simple/\u0026#34; default = true [build-system] requires = [\u0026#34;poetry\u0026gt;=0.12\u0026#34;] build-backend = \u0026#34;poetry.masonry.api\u0026#34; ","date":"2019-09-14T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/poetry%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/","title":"Poetry安装和使用"},{"content":"Nginx (engine x) 是一个高性能的HTTP和反向代理web服务器，同时也提供了IMAP/POP3/SMTP服务。Nginx是由伊戈尔·赛索耶夫为俄罗斯访问量第二的Rambler.ru站点（俄文：Рамблер）开发的，第一个公开版本0.1.0发布于2004年10月4日。 其将源代码以类BSD许可证的形式发布，因它的稳定性、丰富的功能集、示例配置文件和低系统资源的消耗而闻名。2011年6月1日，nginx 1.0.4发布。 Nginx是一款轻量级的Web 服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器，在BSD-like 协议下发行。其特点是占有内存少，并发能力强\nInstall   platform: Centos7\n  version: 7.2\n  安装\nwget http://nginx.org/download/nginx-1.16.1.tar.gz tar -zxvf nginx-1.16.1.tar.gz cd nginx-1.16.1 sudo ./configure \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install whereis nginx # 查看nginx安装地址 /usr/local/nginx   BasicUse   启动\n/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf   重启\ncd /usr/local/nginx/sbin ./nginx -s reload   Example Conf # /usr/local/nginx/conf/nginx.conf  #user nobody; worker_processes 1; #error_log logs/error.log; #error_log logs/error.log notice; #error_log logs/error.log info;  #pid logs/nginx.pid;  events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; #log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39;  # \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39;  # \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;;  #access_log logs/access.log main;  sendfile on; #tcp_nopush on;  #keepalive_timeout 0;  keepalive_timeout 65; #gzip on;  # 包含aps的nginx配置  include /usr/local/nginx/conf/aps/*.conf; server { listen 80; server_name localhost; #charset koi8-r;  #access_log logs/host.access.log main;  location / { root html; index index.html index.htm; } #error_page 404 /404.html;  # redirect server error pages to the static page /50x.html  #  error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } # proxy the PHP scripts to Apache listening on 127.0.0.1:80  #  #location ~ \\.php$ {  # proxy_pass http://127.0.0.1;  #}  # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000  #  #location ~ \\.php$ {  # root html;  # fastcgi_pass 127.0.0.1:9000;  # fastcgi_index index.php;  # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name;  # include fastcgi_params;  #}  # deny access to .htaccess files, if Apache\u0026#39;s document root  # concurs with nginx\u0026#39;s one  #  #location ~ /\\.ht {  # deny all;  #}  } # another virtual host using mix of IP-, name-, and port-based configuration  #  #server {  # listen 8000;  # listen somename:8080;  # server_name somename alias another.alias;  # location / {  # root html;  # index index.html index.htm;  # }  #}  # HTTPS server  #  #server {  # listen 443 ssl;  # server_name localhost;  # ssl_certificate cert.pem;  # ssl_certificate_key cert.key;  # ssl_session_cache shared:SSL:1m;  # ssl_session_timeout 5m;  # ssl_ciphers HIGH:!aNULL:!MD5;  # ssl_prefer_server_ciphers on;  # location / {  # root html;  # index index.html index.htm;  # }  #} } # /usr/local/nginx/conf/aps/aps.conf server { listen 10050; server_name localhost; # 访问后端api  location /api/ { proxy_pass http://127.0.0.1:5500/; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } # 访问静态文件  location /static/ { alias /usr/local/nginx/html/aps/dist/; # 静态文件访问硬盘  } # 访问主页  location / { root /usr/local/nginx/html/aps/dist/; index index.html index.htm; } } ","date":"2019-08-26T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/nginx%E5%AE%89%E8%A3%85%E5%92%8C%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","title":"Nginx安装和基本使用"},{"content":"基本安装  使用rufus以dd模式写入U盘 从u盘启动 将manjaro启动项中的driver和boot添加或修改driver=intel才能进入安装界面(双显卡笔记本) 安装系统, 注意不要联网, 否则容易卡在安装 reboot进入系统 manjaro启动项中quiet后增加nouveau.modeset=0(双显卡)  双显卡使用prime管理连接外接显示器   删除bumblebee或者开源驱动\nsudo mhwd -r pci nonfree 0300   安装nvidia私有闭源驱动\n  方法一:\nsudo mhwd -i pci video-nvidia 或\nsudo mhwd -i pci video-nvidia-390xx # 390xx或者435xx, 数字是驱动版本...   方法二 系统设置-硬件设定中右键安装video-nvidia-390xx之类的驱动\n    安装依赖\nsudo pacman -S linuxXXX-headers acpi_call-dkms xorg-xrandr xf86-video-intel git  注: XXX 为内核版本， 本来我的5.3有点问题，降级成4.19才可以，以4.19为例便是 linux419-headers\n   注入\nsudo modprobe acpi_call   使用github上的脚本\ncd ~ # 建议在用户目录下操作 git clone https://github.com/dglt1/optimus-switch-sddm.git cd optimus-switch-sddm chmod +x install.sh sudo ./install.sh   reboot\n  ","date":"2019-08-01T22:21:57+08:00","permalink":"https://huangkai1008.github.io/p/manjaro%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/","title":"Manjaro安装配置"},{"content":"Git提交代码时需要提交Message, 为了使得提交信息更清晰明了, 需要确定规范\n现在比较流行的规范是Angular规范, 也根据此规范衍生了Conventional Commits specification\n规范 格式  \u0026lt;type\u0026gt;(\u0026lt;scope\u0026gt;): \u0026lt;subject\u0026gt; \u0026lt;BLANK LINE\u0026gt; \u0026lt;body\u0026gt; \u0026lt;BLANK LINE\u0026gt; \u0026lt;footer\u0026gt; 按照空行分割为三个部分, 分别为Header，Body 和 Footer 其中，Header 是必需的，Body 和 Footer 可以省略 不管是哪一个部分，任何一行都不得超过72个字符（或100个字符）, 这是为了避免自动换行影响美观\n组成 Header Header部分只有一行，包括三个字段：type（必需）、scope（可选）和subject（必需）\n  type\n​type用于说明 commit 的类别，只允许使用下面7个标识\n  feat：新功能（feature） fix：修补bug docs：文档（documentation） style： 格式（不影响代码运行的变动） refactor：重构（即不是新增功能，也不是修改bug的代码变动） test：增加测试 chore：构建过程或辅助工具的变动   如果type为feat和fix，则该 commit 将肯定出现在 Change log 之中。其他情况（docs、chore、style、refactor、test）由你决定，要不要放入 Change log，建议是不要\n  scope\nscope用于说明 commit 影响的范围，比如数据层、控制层、视图层等等，视项目不同而不同\n  subject\nsubject是 commit 目的的简短描述，不超过50个字符\n  以动词开头，使用第一人称现在时，比如change，而不是changed或changes 第一个字母小写 结尾不加句号     Body Body 部分是对本次 commit 的详细描述，可以分成多行\n 使用第一人称现在时，比如使用change而不是changed或changes\n  应该说明代码变动的动机，以及与以前行为的对比\n Footer Footer部分可以用于表达不兼容变动和关闭Issue\n  不兼容变动\n如果当前代码与上一个版本不兼容，则 Footer 部分以BREAKING CHANGE开头，后面是对变动的描述、以及变动理由和迁移方法\n  关闭Issue\n Closes APS-151\n   Jetbrains工具配置   git commit template\n 提交信息模板\n   Gitmoji\n 添加emoji表情在commit信息中\n   ","date":"2019-07-12T14:14:15+08:00","permalink":"https://huangkai1008.github.io/p/git-commit-message%E7%BC%96%E5%86%99%E8%A7%84%E8%8C%83/","title":"Git Commit Message编写规范"},{"content":"Web Frameworks  Uvicorn 基于asyncio开发的一个轻量级高效的 web 服务器框架 Starlette Quart Responder Fastapi Sanic  Utils  Poetry 新的Python依赖包管理工具 Pipenv 用了很久的现在也在用的\u0026hellip; 有时候Locking速度感人, pipfile声明版本可以防止很多坑 Black 代码格式化库 Loguru python日志库 PySnooper python Debugger  ORM  Gino tortoise-orm  Test  locust 压力测试工具  Environment  python-dotenv environs  ","date":"2019-05-27T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/%E5%80%BC%E5%BE%97%E5%85%B3%E6%B3%A8%E7%9A%84python%E5%BA%93/","title":"值得关注的Python库"},{"content":"使用IDEA初始化Spring Boot项目   选择File -\u0026gt; New -\u0026gt; Project 新建项目\n  选择Spring Initializr， 点击Next，填写项目基本信息   项目依赖勾选Spring Web选择Finish等待项目构建\n  ​\n","date":"2019-02-07T11:15:10+08:00","permalink":"https://huangkai1008.github.io/p/spring%E5%88%9D%E5%A7%8B%E5%8C%96%E5%B7%A5%E5%85%B7/","title":"Spring初始化工具"},{"content":"What is wsgi **CGI(通用网关接口， Common Gateway Interface/CGI)** **CGI是定义客户端与web服务器交流方式的程序**。\u0026lt;u\u0026gt;例如正常情况下客户端发来一个请求，根据HTTP协议Web服务器将请求内容解析出来，进过计算后，再将加us安出来的内容封装好，例如服务器返回一个HTML页面，并且根据HTTP协议构建返回内容的响应格式。涉及到TCP连接、HTTP原始请求和相应格式的这些，都由一个软件来完成，这时，以上的工作需要一个程序来完成，而这个程序便是CGI\u0026lt;/u\u0026gt;** **WSGI(Web服务器网关接口(Python Web Server Gateway Interface，WSGI)** `WSGI`就是基于`Python`的以`CGI`为标准做一些扩展的协议  What is uwsgi uWSGI，是指实现了WSGI协议的一个web服务器。即用来接受客户端请求，转发响应的程序\nWhat is asgi 异步网关协议接口，一个介于网络协议服务和`Python`应用之间的标准接口，能够处理多种通用的协议类型，包括`HTTP`，`HTTP2`和`WebSocket`  ASGI尝试保持在一个简单的应用接口的前提下，提供允许数据能够在任意的时候、被任意应用进程发送和接受的抽象。并且同样描述了一个新的，兼容HTTP请求响应以及WebSocket数据帧的序列格式。允许这些协议能通过网络或本地socket进行传输，以及让不同的协议被分配到不同的进程中\nDifference between wsgi \u0026amp; asgi 1. Wsgi is based on `Http`, not support `websocket` 2. Asgi is the extension of wsgi.  ","date":"2019-01-24T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/wsgi-asgi-uwsgi%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"WSGI ASGI UWSGI的区别"},{"content":"RabbitMQ  Platform: Centos7  安装   install Erlang\nyum install erlang   install rabbitMQ\n# rpm安装 wget https://github.com/rabbitmq/rabbitmq-server/releases/download/rabbitmq_v3_6_14/rabbitmq-server-3.6.14-1.el7.noarch.rpm yum install rabbitmq-server-3.6.14-1.el7.noarch.rpm # yum安装 yum install rabbitmq-server   配置   启动远程访问\n[{rabbit, [ {loopback_users, []} ]}]   安装插件\n/sbin/rabbitmq-plugins enable rabbitmq_management   使用   服务命令\nsystemctl start rabbitmq-server.service # 启动 systemctl status rabbitmq-server.service\t# 查看状态 systemctl restart rabbitmq-server.service\t# 重启 systemctl enable rabbitmq-server.service # 开机自启   添加用户\nrabbitmqctl add_user root root123 # 添加新用户，用户名为 \u0026#34;root\u0026#34; ，密码为 \u0026#34;root123\u0026#34; rabbitmqctl set_permissions -p / root \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; # 为root用户添加所有权限 rabbitmqctl set_user_tags root administrator # 设置root 用户为管理员角色   访问web页面\nhttp://ip:15672   ","date":"2018-11-12T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/rabbitmq%E5%9F%BA%E7%A1%80%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8/","title":"RabbitMQ基础安装使用"},{"content":"mysql中min和max查询优化  max()函数需扫描where条件过滤后的所有行\n  慎用max()函数，特别是频繁执行的sql，若需用到可转化为order by id desc limit 1\n ","date":"2018-11-06T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/mysql%E4%BC%98%E5%8C%96/","title":"Mysql优化"},{"content":"Redis 安装   Platform: centos7\n  version: 5.0\n  安装\nwget http://download.redis.io/releases/redis-5.0.0.tar.gz # 获取包 tar -zxvf redis-5.0.0.tar.gz mv redis-5.0.0 /usr/local/redis make \u0026amp;\u0026amp; make install   Redis配置   设置配置文件目录\nmkdir -p /etc/redis cp redis.conf /etc/redis   修改配置文件\nvim /etc/redis/redis.conf daemonize yes (no -\u0026gt; yes) # 守护进程 bind 0.0.0.0 (127.0.0.1 -\u0026gt; 0.0.0.0) # 远程登录 protected-mode no (yes -\u0026gt; no) # 关闭保护模式/或者添加密码   Redis使用   启动\n/usr/local/bin/redis-server /etc/redis/redis.conf   查看启动\nps -ef | grep redis   客户端使用\nredis-cli # 进入 127.0.0.1:6379\u0026gt;set name Huang Ok redis-cli shutdown # 关闭客户端   开机启动配置\n# 开机启动要配置在 rc.local 中，而 /etc/profile 文件，要有用户登录了，才会被执行。 echo \u0026#34;/usr/local/bin/redis-server /etc/redis/redis.conf \u0026amp;\u0026#34; \u0026gt;\u0026gt; /etc/rc.local   Supervisor管理Redis   更改redis配置\nvim /etc/redis/redis.conf daemonize no (yes -\u0026gt; no) # 取消守护进程   创建supervisor对redis的配置文件\nvim /etc/supervisord.d/redis.ini  `redis.ini`文件如下  [program:redis] command=redis-server /etc/redis/redis.conf\t#\t启动Redis的命令 autostart=true\t#\tsupervisord启动时，该程序也启动 autorestart=true # 异常退出时，自动启动 startsecs=3\t# 启动后持续3s后未发生异常，才表示启动成功\t stdout_logfile=/var/log/supervisor/redis/redis.log # 标准输出流日志 stderr_logfile=/var/log/supervisor/redis/redis_err.log\t# 标准错误输出流日志   ","date":"2018-10-11T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/redis%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/","title":"Redis安装配置"},{"content":"安装Angular 确保node/npm已安装 node -v 查看node版本 npm -v 查看npm版本 安装typescript npm install -g typescript 安装Angular CLI npm install -g @angular/cli ng version # 验证angular-cli版本 建立一个新的Angular项目 Angular CLI 为我们提供了两种方式，用于创建新的应用程序：\n  ng init - 在当前目录创建新的应用程序\n  ng new - 创建新的目录，然后在新建的目录中运行 ng init 命令\n因此 ng new 与 ng init 的功能是相似的，只是 ng new 会为我们创建新的目录\n  创建应用 ng new my-app 可用选项  --dry-run: boolean, 默认为 false, 若设置 dry-run 则不会创建任何文件 --verbose: boolean, 默认为 false --link-cli: boolean, 默认为 false, 自动链接到 @angular/cli 包 --skip-install: boolean, 默认为 false, 表示跳过 npm install --skip-git: boolean, 默认为 false, 表示该目录不初始化为 git 仓库 --skip-tests: boolean, 默认为 false, 表示不创建 tests 相关文件 --skip-commit: boolean, 默认为 false, 表示不进行初始提交 --directory: string, 用于设置创建的目录名，默认与应用程序的同名 --source-dir: string, 默认为 'src', 用于设置源文件目录的名称 --style: string, 默认为 'css', 用于设置选用的样式语法 ('css', 'less' or 'scss') --prefix: string, 默认为 'app', 用于设置创建新组件时，组件选择器使用的前缀 --mobile: boolean, 默认为 false,表示是否生成 Progressive Web App 应用程序 --routing: boolean, 默认为 false, 表示新增带有路由信息的模块，并添加到根模块中 --inline-style: boolean, 默认为 false, 表示当创建新的应用程序时，使用内联样式 --inline-template: boolean, 默认为 false, 表示当创建新的应用程序时，使用内联模板  ","date":"2018-10-09T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/angular-starter/","title":"Angular Starter"},{"content":"安装  Platform: centos7  ","date":"2018-09-12T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/mongodb%E7%9A%84%E5%AE%89%E8%A3%85/","title":"MongoDB的安装"},{"content":"MariaDB安装   platform: Centos7\n  Install\nyum install -y mariadb-server   MariaDB配置使用   Using\nsystemctl start mariadb.service # 启动 systemctl enable mariadb.service # 开机自启   Configure\n  首先是设置密码，会提示先输入密码\n Enter current password for root (enter for none): \u0026lt;–直接回车\n  Set root password? [Y/n] \u0026lt;– 是否设置root用户密码，输入y并回车或直接回车\n  New password: \u0026lt;– 设置root用户的密码\n  Re-enter new password: \u0026lt;– 再输入一次你设置的密码\n  其他配置\n  Remove anonymous users? [Y/n] \u0026lt;– 是否删除匿名用户，Y回车\n  Disallow root login remotely? [Y/n] \u0026lt;–是否禁止root远程登录, N回车,\n  Remove test database and access to it? [Y/n] \u0026lt;– 是否删除test数据库，Y回车\n  Reload privilege tables now? [Y/n] \u0026lt;– 是否重新加载权限表，Y回车2.开启远程访问\n   开启远程访问\n  GRANT ALL PRIVILEGES ON *.* TO \u0026#39;root\u0026#39;@\u0026#39;%\u0026#39;IDENTIFIED BY \u0026#39;123456\u0026#39; WITH GRANT OPTION;  刷新权限  flush privileges  配置文件地址     ","date":"2018-09-09T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/mariadb%E5%AE%89%E8%A3%85/","title":"MariaDB安装"},{"content":"Mysql安装  版本: 8.0 添加源 yum local install https://repo.mysql.com//mysql80-community-release-el7-1.noarch.rpm  安装 yum install mysql-community-server   Mysql配置   初始化\nsudo mysqld --initialize --user=mysql --basedir=/usr --datadir=/var/lib/mysql   启动mysql\nsystemctl start mysqld   设置mysql开机自启\nsystemctl enable mysqld   查看初始密码\ngrep \u0026#39;temporary password\u0026#39; /var/log/mysqld.log   进入mysql\nmysql -u root -p   修改密码\nALTER USER \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;Huang|12345\u0026#39;   查看版本\nselect version(); +-----------+ | version() | +-----------+ | 8.0.16 | +-----------+ 1 row in set (0.00 sec)   查看端口\nshow global variables like \u0026#39;port\u0026#39;; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | port | 3306 | +---------------+-------+ 1 row in set (0.04 sec)   远程访问\nuse mysql; update user set host = \u0026#39;%\u0026#39; where user = \u0026#39;root\u0026#39;; flush privileges   ","date":"2018-09-09T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/","title":"Mysql安装配置"},{"content":"数据库系统的目的(Purpose of Database Systems) 在早期，数据库应用程序直接建立在文件系统之上，导致一系列的问题\n  数据冗余和不一致(Data redundancy and inconsistency)\n  数据访问难度大(Difficulty in accessing data)\n  数据隔离(Data isolation)\n  完整性问题(Integrity problems)\n 完整性约束(Integrity constraints)问题\n难以添加新约束和修改约束\n   原子性更新(Atomicity of updates)\n 更新失败可能会导致数据库的数据处于不一致的状态，或者只更新了部分数据\n例如: 从一方转账给另一方，只会有完成转账和完全没发生转账两种情况，不会出现转账方转账了但是收款方未收到款项的问题\n   多用户并发访问(Concurrent access by multiple users)\n 并发访问需要高性能的支持， 而不受控制的并发访问可能会导致数据不一致\n   安全问题(Security problems)\n 文件系统难以提供安全保障\n 数据库系统就是为了解决这些问题产生的\n  数据模型(Data Models) 组成  一系列用于描述的工具    数据(Data)\n  数据关系(Data relationships)\n 数据语义(Data semantics) 数据约束(Data constraints)     关系模型(Relational model) 实体关系数据模型(Entity-Relationship data model 主要用于数据库设计) 基于对象的数据模型(Object-based data models (Object-oriented and Object-relational)) 半结构化数据模型(Semi-structured data model (XML))  数据视图(View of Data) 一个数据库系统的结构如下图 模式与实例(Instances and Schema) 类似于编程语言中的类型和变量\n 逻辑模式(logic schema) 数据库的总体逻辑结构，类似于程序设计中的变量类型信息 物理模式(physical schema) 数据库的总体物理结构 实例(instance) 数据库在特定时间点的实际内容， 类似于变量的值  物理数据独立性(Physical Data Independence)  定义： 在不更改逻辑模式的情况下修改物理模式的能力\n   应用程序依赖于逻辑模式(logic schema)\n  一般来说，不同级别和组件应该定义得很好，以便在某些部分中进行更改，不严重影响他人\n  数据定义语言(Data Definition Language)  定义数据库模式的规范表示法\n create table instructor ( ID char(5), name varchar(20), dept_name varchar(20), salary numeric(8,2)) DDL编译器生成一组存储表模板信息的数据字典（data dictionary)\n数据字典包含元信息(metadata)\n 数据库模式(database schema) 完整性约束(Integrity constraints)  主键   授权(Authorization)  数据处理语言(Data Manipulation Language )  用于访问和更新由适当数据模型组织的数据的语言（查询语言）\n  DML一般分为两种类型  Pure Commercial  例如SQL      ​\n结构化查询语言(Structured Query Language, SQL)  SQL查询语言是非过程的查询将多个表（可能只有一个）作为输入，并始终返回一个表(SQL query language is nonprocedural. A query takes as input several tables (possibly only one) and always returns a single table)\n 数据库设计(Database Design)   逻辑设计(logic design) \u0026ndash; 决定数据库模式\n  业务决定\n 我们应该在数据库中记录哪些属性\n   计算机科学决定\n  我们应该有什么关系模式     属性应该如何分布在不同的关系模式中       物理设计(physical design) \u0026ndash; 决定数据库的物理布局\n  ","date":"2018-09-09T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%AE%BA-%E4%BB%8B%E7%BB%8D/","title":"数据库系统概论-介绍"},{"content":"安装   Platform: Centos7\n  version: 1.12\n  安装\ncd /opt wget https://studygolang.com/dl/golang/go1.12.4.linux-amd64.tar.gz tar xzvf go1.12.4.linux-amd64.tar.gz\t# 安装   配置环境变量\nvim ~/.zshrc\t# 如果用bash就是vim ~/.bashrc # 追加golang配置 export GOROOT=/opt/go export PATH=$PATH:$GOROOT/bin # 立即生效 source ~/.zshrc # 查看版本 go version   ","date":"2018-07-31T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/golang%E5%AE%89%E8%A3%85/","title":"Golang安装"},{"content":"Python默认的json模块序列化并不是很全面，只能序列化基本的数据类型, 像一些时间格式或者自定义类型都不能序列化，所以在有些时候需要扩展json模块的json encoder\n扩展 import datetime as dt import decimal import json import enum from collections.abc import Iterator class ExtendedEncoder(json.JSONEncoder): def default(self, o): if isinstance(o, dt.datetime): return o.strftime(\u0026#39;%Y-%m-%d%H:%M:%S\u0026#39;) elif isinstance(o, dt.date): return o.strftime(\u0026#39;%Y-%m-%d\u0026#39;) elif isinstance(o, decimal.Decimal): return float(o) elif isinstance(o, Iterator): return list(o) elif isinstance(o, enum.Enum): return o.value return json.JSONEncoder.default(self, o) 使用场景   日常格式化\n例如对于日期格式的格式化\nimport datetime as dt now = dt.datetime.now() 对于now如果使用json.dumps(t_now)便会触发TypeError: Object of type datetime is not JSON serializable使用扩展的Encoder\n\u0026gt;\u0026gt;\u0026gt; json.dumps(now, cls=ExtendedEncoder) \u0026#39;2018-04-09 23:04:49\u0026#39;   Flask\n修改flask类的json_encoder\nfrom flask import Flask as _Flask class QuizFlask(_Flask): \u0026#34;\u0026#34;\u0026#34; 自定义flask \u0026#34;\u0026#34;\u0026#34; json_encoder = ExtendedEncoder def make_response(self, rv): if rv is None: rv = dict() if isinstance(rv, Iterator): rv = list(rv) return super(QuizFlask, self).make_response(rv)   Tortoise-orm\n模型jsonfield的encoder\nimport json from tortoise import fields from tortoise.models import Model __all__ = [\u0026#39;OurModel\u0026#39;] class OurModel(Model): \u0026#34;\u0026#34;\u0026#34;示例model\u0026#34;\u0026#34;\u0026#34; id = fields.IntField(pk=True) cat_ids = fields.JSONField( encoder=ExtendedEncoder, decoder=json.decoder ) # JsonField的encoder   ","date":"2018-04-09T22:49:14+08:00","permalink":"https://huangkai1008.github.io/p/%E6%89%A9%E5%B1%95python-json-encoder/","title":"扩展Python Json Encoder"},{"content":"介绍 从Python3.2开始，标准库为我们提供了 concurrent.futures 模块，它提供了 ThreadPoolExecutor (线程池)和ProcessPoolExecutor (进程池)两个类。\n相比 threading 等模块，该模块通过 submit 返回的是一个 future 对象，它是一个未来可期的对象，通过它可以获悉线程的状态主线程(或进程)中可以获取某一个线程(进程)执行的状态或者某一个任务执行的状态及返回值：\n1.主线程可以获取某一个线程（或者任务的）的状态，以及返回值。\n2.当一个线程完成的时候，主线程能够立即知道。\n3.让多线程和多进程的编码接口一致。\n基本使用 from concurrent.futures import ThreadPoolExecutor import time def get_page(url): time.sleep(url) return url with ThreadPoolExecutor(max_workers=5) as t: # 创建一个最大容纳数量为5的线程池 task1 = t.submit(get_page, 1) task2 = t.submit(get_page, 2) # 通过submit提交执行的函数到线程池中 task3 = t.submit(get_page, 3) print(f\u0026#34;task1: {task1.done()}\u0026#34;) # 通过done来判断线程是否完成 print(f\u0026#34;task2: {task2.done()}\u0026#34;) print(f\u0026#34;task3: {task3.done()}\u0026#34;) time.sleep(2.5) print(f\u0026#34;task1: {task1.done()}\u0026#34;) print(f\u0026#34;task2: {task2.done()}\u0026#34;) print(f\u0026#34;task3: {task3.done()}\u0026#34;) print(task1.result()) # 通过result来获取返回值 \u0026gt;\u0026gt;\u0026gt; task1: False task2: False task3: False ... task1: True task2: True task3: False Api as_completed  concurrent.futures.as_completed(fs, timeout=None)\n  返回一个生成器在迭代过程中会阻塞\n  直到线程完成或者异常时,返回一个被set_result的Future对象\n  此方法的返回顺序为哪个线程先失败/完成就返回\n from concurrent.futures import ThreadPoolExecutor, as_completed import time def get_page(url): time.sleep(url) return url with ThreadPoolExecutor(max_workers=5) as t: # 创建一个最大容纳数量为5的线程池 tasks = [t.submit(get_page, page) for page in range(1, 5)] for future in as_completed(tasks): result = future.result() print(result) \u0026gt;\u0026gt;\u0026gt; 1 2 3 4 wait  concurrent.futures.wait(fs, timeout=None, return_when=ALL_COMPLETED)\n  fs: 执行的序列\n  timeout: 等待的最大时间，如果超过这个时间即使线程未执行完成也将返回\n  return_when: 表示wait返回结果的条件，默认为 ALL_COMPLETED 全部执行完成再返回\n   FIRST_COMPLETED   函数将在任意可等待对象结束或取消时返回。    FIRST_EXCEPTION   函数将在任意可等待对象因引发异常而结束时返回。 当没有引发任何异常时它就相当于 ALL_COMPLETED。    ALL_COMPLETED   函数将在所有可等待对象结束或取消时返回。  from concurrent.futures import ThreadPoolExecutor, wait import time def get_page(url): time.sleep(url) return url with ThreadPoolExecutor(max_workers=5) as t: # 创建一个最大容纳数量为5的线程池 tasks = [t.submit(get_page, page) for page in range(1, 5)] a, b = wait(tasks) print(a) print(b) \u0026gt;\u0026gt;\u0026gt; {\u0026lt;Future at 0x1c071fb1f28 state=finished returned int\u0026gt;, \u0026lt;Future at 0x1c071fb1d68 state=finished returned int\u0026gt;, \u0026lt;Future at 0x1c071f9fd68 state=finished returned int\u0026gt;, \u0026lt;Future at 0x1c071d78278 state=finished returned int\u0026gt;} set() map  *concurrent.futures.Executor.map(fn, iterables, timeout=None)\n  fn: 第一个参数 fn 是需要线程执行的函数\n  *iterables: 第二个参数接受一个可迭代对象\n  timeout: 第三个参数 timeout 跟 wait() 的 timeout 一样，但由于 map 是返回线程执行的结果，如果 timeout小于线程执行时间会抛异常 TimeoutError\n from concurrent.futures import ThreadPoolExecutor import time def get_page(url): time.sleep(url) return url URLS = [url for url in range(1, 4)] with ThreadPoolExecutor(max_workers=5) as executor: # 创建一个最大容纳数量为5的线程池 for result in executor.map(get_page, URLS): print(result) \u0026gt;\u0026gt;\u0026gt; 1 2 3 回调函数 回调函数(add_done_callback)是在调用线程完成后再调用的\nfrom concurrent.futures import ThreadPoolExecutor, wait import threading import time def get_page(url): time.sleep(url) return url def call_back(worker): print(f\u0026#39;tid: {threading.current_thread().ident}\u0026#39;, worker.result()) with ThreadPoolExecutor() as t: tasks = [] for page in range(1, 5): task = t.submit(get_page, url=page) task.add_done_callback(call_back) tasks.append(task) wait(tasks) \u0026gt;\u0026gt;\u0026gt; tid: 6392 1 tid: 14936 2 tid: 12516 3 tid: 10524 4 异常处理  通过添加回调函数的方法处理异常  import logging def executor_callback(worker): logging.info(f\u0026#39;finished\u0026#39;) worker_exception = worker.exception() if worker_exception: logging.exception(worker_exception) 备注  一定使用with关键字处理线程池，在某些情况下线程池可能不能自动回收线程资源，with可以避免内存持续增长等情况  ","date":"2018-02-08T22:49:14+08:00","permalink":"https://huangkai1008.github.io/p/python%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%BD%BF%E7%94%A8/","title":"Python线程池使用"},{"content":"  Platform: centos7\n  version: 5.0\n  安装   Uninstall old versions\nsudo yum remove docker \\  docker-client \\  docker-client-latest \\  docker-common \\  docker-latest \\  docker-latest-logrotate \\  docker-logrotate \\  docker-engine   Install Docker CE\nsudo yum install -y yum-utils \\  device-mapper-persistent-data \\  lvm2 # 设置stable源 sudo yum-config-manager \\  --add-repo \\  https://download.docker.com/linux/centos/docker-ce.repo # 安装Docker CE sudo yum install docker-ce docker-ce-cli containerd.io   启动  Docker启动 sudo systemctl start docker\t# 启动Docker sudo systemctl status docker\t# 查看Docker状态   ","date":"2018-01-31T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/docker%E5%AE%89%E8%A3%85/","title":"Docker安装"},{"content":"  遇到in查询之类的批量删除或者更新，可以使用synchronize_session=False\ndb.session.delete(synchronize_session=False)   使用find_in_set\nfrom sqlalchemy.sql.expression import func db.session.query(Post).filter(func.find_in_set(\u0026#39;10\u0026#39;, Post.c.tag_id))   批量增加删除\ndb.session.add_all(instances) db.session.delete_all(instances)   Mysql IS NULL判断\nisnot() is_()   Mysql 联合主键\nfrom sqlalchemy import PrimaryKeyConstraint class Node(Model): __table_args__ = ( PrimaryKeyConstraint(\u0026#39;pk1\u0026#39;, \u0026#39;pk2\u0026#39;), )   Flask_sqlalchemy支持Double精度类型字段\nfrom sqlalchemy import Column from sqlalchemy.dialects.mysql import DOUBLE from app import db class BaseModel(Model): id = db.Column(db.Integer, primary_key=True) # Flask_sqlalchemy double_column = Column(DOUBLE, comment=\u0026#39;双精度字段\u0026#39;) # Sqlalchemy mysql double column   subquery使用实例\nconditions = list() for key, value in material_period.items(): condition = and_( CraftEntityAttrs.attr_number == key, CraftEntityAttrs.attr_value == value ) conditions.append(condition) if not conditions: return list() stmt = ( db.session.query(CraftEntityAttrs.entity_id, CraftEntityAttrs.cat_number) .filter(or_(*conditions)) .subquery() ) query = db.session.query( CraftEntityPeriodHours.proc_number, CraftEntityPeriodHours.period, CraftEntityPeriodHours.hours, CraftEntityPeriodHours.major_wrapper_skill_level, stmt.c.cat_number, ).filter(CraftEntityPeriodHours.entity_id == stmt.c.entity_id) stmt = ( db.session.query(ProducePlan.row_id, ProducePlan.row_seq) .filter(ProducePlan.proc_number.in_(constants.COIL_PROC_NUMBERS)) .distinct() .subquery() ) query = ( BatchDetail.query.join( stmt, and_( BatchDetail.row_id == stmt.c.row_id, BatchDetail.row_seq == stmt.c.row_seq, ), ) .join(PlanRow, BatchDetail.row_id == PlanRow.id) .join(RowProject, PlanRow.project_id == RowProject.id) .join(Order, RowProject.order_id == Order.id) .with_entities( Order.order_number, Order.id.label(\u0026#39;order_id\u0026#39;), Order.project_name, RowProject.row_project_number, RowProject.id.label(\u0026#39;project_id\u0026#39;), Order.purchase_unit, RowProject.fac_number, RowProject.mat_number, RowProject.mat_desc, PlanRow.com_qty, BatchDetail.row_id, PlanRow.plan_row_number, BatchDetail.batch_id, BatchDetail.batch_number, BatchDetail.batch_qty, BatchDetail.batch_seq, BatchDetail.single_pack_cycle, ) .order_by(RowProject.id, BatchDetail.batch_id, BatchDetail.batch_seq) ) stmt = ( db.session.query(ProducePlan.project_id) .outerjoin( ProducePlanCompletion, ProducePlan.plan_id == ProducePlanCompletion.plan_id ) .filter( or_( ProducePlanCompletion.completion.is_(None), ProducePlanCompletion.completion == constants.ProducePlanCompletion.not_scheduled.value, ) ) .distinct() .subquery() ) query = db.session.query(ProducePlan.project_id).filter( ProduceUserPlan.project_id.in_(stmt), ProduceUserPlan.proc_type == \u0026#39;design\u0026#39; )   ","date":"2017-06-14T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/sqlalchemy%E4%BD%BF%E7%94%A8%E6%B3%A8%E6%84%8F/","title":"Sqlalchemy使用注意"},{"content":"Python安装   Platform: centos7\n  Version: 3.7\n  安装编译环境\nyum install zlib-devel bzip2-devel openssl-devel ncurses-devel libffi-devel   下载\nwget --no-check-certificate https://www.python.org/ftp/python/3.7.4/Python-3.7.4.tgz   创建安装目录解压\nsudo mkdir /usr/local/python3 tar -zxvf Python-3.7.4.tgz cd Python-3.7.4/   编译安装\nsudo ./configure --prefix=/usr/local/python3 # 指定创建的目录 make \u0026amp;\u0026amp; make install # 编译安装   软链接  创建python和pip软链接 ln -s /usr/local/python3/bin/python3 /usr/bin/python3 # python3 软链接 ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3 # pip3软链接 ln -s /usr/local/python3/bin/pipenv /usr/bin/pipenv # pipenv软链接   使用pyenv管理多个Python版本 安装pyenv  安装脚本 curl https://pyenv.run | bash  manjaro sudo pacman -S pyenv   pyenv基本使用  展示可以安装的版本 pyenv install --list  安装python pyenv install 3.7.4  manjaro如遇到ModuleNotFoundError: No module named \u0026lsquo;_ctypes\u0026rsquo;, 可执行sudo pacman -S pkgconf libffi\n  查看可使用的版本，带*表示当前使用的版本 $ pyenv versions * system (set by /home/huangkai/.pyenv/version) 3.7.4  配置及管理python版本  使用pyenv global 配置当前用户的系统使用的python版本 使用pyenv shell 配置当前shell的python版本，退出shell则失效 使用pyenv local 配置所在项目（目录）的python版本    ","date":"2017-01-08T22:49:14+08:00","permalink":"https://huangkai1008.github.io/p/python%E5%AE%89%E8%A3%85/","title":"Python安装"}]